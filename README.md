
# Reduced Graphs

All of the reduced graphs generated by this app are in CSV format. There will be two for each year's crawl: one formed from the top 30,000 nodes sorted by
in-degree, and another form the top 10,000 of the first subgraph sorted by PageRank.

The reduced data can be found [here](https://drive.google.com/drive/folders/1odjh6_URj1K8rUjA6yI06YoV7yLSN79e?dmr=1&ec=wgc-drive-hero-goto).

# Data Source

All data files should be found in the top-level `data` directory. They are omitted from this repository, so for the sake of reproducibility the steps taken to obtain/format them are listed here.

1. The following files were downloaded from [CommonCrawl's 2018 web crawl](https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2018-19-nov-dec-jan/index.html).
    - `cc-main-2018-19-nov-dec-jan-host-vertices.paths.gz`: This file contains a list of paths to download 42 separate compressed text files. Each of these files were downloaded, uncompressed, and then concatenated into a single text file named `2018-node-domains.txt`. Since the files are fairly large, it's not practical to copy+paste the files together. If you are able to run bash scripts, there is one at the top level named `domains.sh`. If you download the paths file for the vertices of a given webgraph, extract it to a text file named "paths-file.txt" then run the following, the paths will automatically be downloaded and concatenated into a proper domains file.
    ```bash
    chmod u+x ./domains.sh;
    ./domains.sh paths-file.txt ./data/XXXX-node-domains.txt;
    ``` 
    - `cc-main-2018-19-nov-dec-jan-host.graph`, `cc-main-2018-19-nov-dec-jan-host.properties`: These two files represent the main webgraph; these files were renamed to `2018.graph` and `2018.properties`, respectively.
    - `cc-main-2018-19-nov-dec-jan-host-t.graph`, `cc-main-2018-19-nov-dec-jan-host-t.properties`: These files represent the transpose of the webgraph; these files were renamed to `2018-t.graph` and `2018-t.properties`, respectively.
2. The following files were downloaded from [CommonCrawl's 2024 web crawl](https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2024-jul-aug-sep/index.html).
    - `cc-main-2024-jul-aug-sep-host-vertices.paths.gz`: This file contains a list of paths to download 15 separate compressed text files. Each of these 15 files were downloaded, uncompressed, and then concatenated into a single text file named `2024-node-domains.txt` (which came out to about 10 GB).
    - `cc-main-2024-jul-aug-sep-host.graph`, `cc-main-2024-jul-aug-sep-host.properties`: These two files represent the main webgraph; these files were renamed to `2024.graph` and `2024.properties`, respectively.
    - `cc-main-2024-jul-aug-sep-host-t.graph`, `cc-main-2024-jul-aug-sep-host-t.properties`: These files represent the transpose of the webgraph; these files were renamed to `2024-t.graph` and `2024-t.properties`, respectively.
3. Similarly to how files were downloaded for 2018 and 2024 data, so too were files downloaded and renamed for [2020 data](https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2020-21-oct-nov-jan/index.html) and [2022 data](https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2021-22-oct-nov-jan/index.html).

All totaled, the `data` directory has 20 distinct files for a combined 100 GB.

# Running the App

Run the following from the CLI to produce the reduced webgraphs.

```
mvn exec:java -Dexec.mainClass="App"
```

If you run out of heap memory or if you need to restrict it, try adjusting it within the `.mvn/jvm.config` file.

```
 -Xmx4g -Xms512m
```

The above setting limits the heap size to a minimum of 512 MB and a maximum of 4 GB.